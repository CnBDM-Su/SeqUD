{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1: SVM for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbde841c2efb4244ac10111fe906e356",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stop!\n",
      "Search completed in 24.08 seconds.\n",
      "The best score is: 0.97719.\n",
      "The best configurations are:\n",
      "C                   : 32.0\n",
      "gamma               : 0.03125\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn import datasets\n",
    "from matplotlib import pylab as plt\n",
    "from sklearn.model_selection import KFold \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from seqmm.pybayopt import GPEISklearn\n",
    "\n",
    "sx = MinMaxScaler()\n",
    "dt = datasets.load_breast_cancer()\n",
    "x = sx.fit_transform(dt.data)\n",
    "y = dt.target\n",
    "\n",
    "ParaSpace = {'C':     {'Type': 'continuous', 'Range': [-6, 16], 'Wrapper': np.exp2}, \n",
    "             'gamma': {'Type': 'continuous', 'Range': [-16, 6], 'Wrapper': np.exp2}}\n",
    "\n",
    "estimator = svm.SVC()\n",
    "score_metric = make_scorer(accuracy_score, True)\n",
    "cv = KFold(n_splits=5, random_state=0, shuffle=True)\n",
    "\n",
    "clf = GPEISklearn(estimator, cv, ParaSpace, max_runs = 100, time_out = 10, refit = True, verbose = True)\n",
    "clf.fit(x, y)\n",
    "clf.plot_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import signal\n",
    "import shutil\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from spearmint.ExperimentGrid import GridMap\n",
    "import spearmint.chooser.GPEIOptChooser as module\n",
    "grid_size = 20000\n",
    "\n",
    "\n",
    "def spmint_opt(chooser, grid, values, grid_status):\n",
    "    \n",
    "    ## The status of jobs, 0 - candidate, 1 - pending, 2 - complete. \n",
    "    ## Here we only have two status: 0 or 2 available. \n",
    "    job_id = chooser.next(grid, np.squeeze(values), [],\n",
    "                          np.nonzero(grid_status == 0)[0],\n",
    "                          np.nonzero(grid_status == 1)[0],\n",
    "                          np.nonzero(grid_status == 2)[0])\n",
    "    return job_id\n",
    "\n",
    "\n",
    "ParaSpace = {'booster':          {'Type': 'categorical', 'Mapping': ['gbtree', 'gblinear']},\n",
    "             'max_depth':        {'Type': 'integer',     'Mapping': np.linspace(2,10,9)}, \n",
    "             'n_estimators':     {'Type': 'integer',     'Mapping': np.linspace(100,500,401)},\n",
    "             'min_child_weight': {'Type': 'integer',     'Mapping': np.linspace(1,100,100)},\n",
    "             'subsample':        {'Type': 'continuous',  'Range': [0, 1],  'Wrapper': lambda x:x},\n",
    "             'colsample_bytree': {'Type': 'continuous',  'Range': [0, 1],  'Wrapper': lambda x:x},\n",
    "             'learning_rate':    {'Type': 'continuous',  'Range': [-5, 0], 'Wrapper': lambda x: 10**x},\n",
    "             'gamma':            {'Type': 'continuous',  'Range': [-5, 0], 'Wrapper': lambda x: 10**x},\n",
    "             'reg_lambda':       {'Type': 'continuous',  'Range': [-5, 0], 'Wrapper': lambda x: 10**x},\n",
    "             'reg_alpha':         {'Type': 'continuous',  'Range': [-5, 0], 'Wrapper': lambda x: 10**x}}\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import KFold \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "estimator = xgb.XGBRegressor()\n",
    "score_metric = make_scorer(mean_squared_error, False)\n",
    "cv = KFold(n_splits=5, random_state=0, shuffle=True)\n",
    "def obj_func(cfg):\n",
    "    next_params = pd.DataFrame(np.array([cfg]), columns = ParaSpace)\n",
    "    parameters = {}\n",
    "    for item, values in ParaSpace.items():\n",
    "        if (values['Type']==\"continuous\"):\n",
    "            parameters[item] = values['Wrapper'](float(next_params[item].iloc[0]))\n",
    "        elif (values['Type']==\"integer\"):\n",
    "            parameters[item] = int(next_params[item].iloc[0]) \n",
    "        elif (values['Type']==\"categorical\"):\n",
    "            parameters[item] = next_params[item][0]\n",
    "    estimator.set_params(**parameters)\n",
    "    out = cross_val_score(estimator, x, y, cv = cv)\n",
    "    score = np.mean(out)\n",
    "\n",
    "    return -score, score, score\n",
    "    \n",
    "dt = datasets.load_diabetes()\n",
    "sx = MinMaxScaler()\n",
    "sy = MinMaxScaler()\n",
    "x = sx.fit_transform(dt.data)\n",
    "y = sy.fit_transform(dt.target.reshape([-1,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "0 gbtree\n",
      "1 [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]\n",
      "0 gbtree\n",
      "20000 [0.53151872 0.5126023  0.55828276 0.70306628 0.5051355  0.50268264\n",
      " 1.         0.91035543 0.52234061 0.99193004]\n",
      "1 gblinear\n",
      "20001 [0.70472684 0.53067201 0.35905349 0.50490099 0.48998212 0.49386614\n",
      " 0.27512002 0.03095751 0.81374183 0.27266062]\n",
      "1 gblinear\n",
      "20002 [0.36399014 0.47144928 0.61743317 0.49596104 0.50950466 0.50108895\n",
      " 0.4978231  0.72040467 0.36901153 0.57301609]\n",
      "0 gbtree\n",
      "20003 [0.39793078 0.27830787 0.58309346 0.23047892 0.7375476  0.50089345\n",
      " 0.50138538 0.71168139 0.23692757 0.52568991]\n",
      "0 gbtree\n",
      "20004 [0.         0.36097738 0.79963174 0.36444964 1.         0.50156653\n",
      " 0.51807454 0.62022023 0.         0.54347659]\n",
      "0 gbtree\n",
      "20005 [0.10720899 0.38223987 1.         0.25946626 1.         1.\n",
      " 0.49673133 1.         0.         0.68162256]\n",
      "0 gbtree\n",
      "20006 [0.         1.         1.         0.         1.         0.\n",
      " 0.         0.82703719 0.         0.        ]\n",
      "0 gbtree\n",
      "20007 [0.         0.         0.98689439 0.55916629 1.         1.\n",
      " 0.96974794 0.03865755 0.         0.7377918 ]\n",
      "0 gbtree\n",
      "20008 [0.         0.         0.23390214 0.         1.         0.69336598\n",
      " 0.43133901 1.         0.         1.        ]\n",
      "0 gbtree\n",
      "20009 [0.1145086  0.31874757 1.         1.         1.         0.7524354\n",
      " 0.51327722 0.59852901 0.         0.49982385]\n",
      "0 gbtree\n",
      "20010 [0.49379798 0.33289124 1.         0.34315242 0.84362875 0.84567283\n",
      " 0.55111713 0.54272963 0.         0.49192308]\n",
      "0 gbtree\n",
      "20011 [0.30802252 0.3477363  0.88395689 0.41449349 0.83662272 0.81833144\n",
      " 0.70275236 0.5514649  0.01904634 0.54594214]\n",
      "0 gbtree\n",
      "20012 [0.28694978 0.37757701 1.         0.12137348 1.         0.70310484\n",
      " 0.61166257 0.37592812 0.22674244 0.51391482]\n",
      "0 gbtree\n",
      "20013 [1.         0.56560306 1.         0.39971499 1.         0.67166858\n",
      " 0.62505736 0.73688634 0.         0.53973527]\n",
      "1 gblinear\n",
      "20014 [0.42097749 0.19423439 1.         0.         1.         1.\n",
      " 0.91456557 0.70488385 0.         0.22973567]\n",
      "0 gbtree\n",
      "20015 [0.22015632 1.         1.         0.         1.         1.\n",
      " 0.8860904  0.60907491 0.         0.45089617]\n",
      "0 gbtree\n",
      "20016 [1.         0.26916818 1.         0.         0.         1.\n",
      " 0.91454296 0.         0.072172   0.39950404]\n",
      "1 gblinear\n",
      "20017 [1.         0.         1.         0.         0.         1.\n",
      " 1.         0.60061816 0.         0.43745377]\n",
      "1 gblinear\n",
      "20018 [1.         0.20496493 1.         0.         0.         1.\n",
      " 0.94054246 0.81449278 0.         0.        ]\n",
      "1 gblinear\n",
      "20019 [1.         0.         1.         0.13373376 0.18876067 1.\n",
      " 1.         0.42664055 0.         0.21376467]\n",
      "1 gblinear\n",
      "20020 [0.8920686  0.         1.         0.         0.02261674 1.\n",
      " 0.78050395 0.42468545 0.         0.21025413]\n",
      "1 gblinear\n",
      "20021 [1.         0.         1.         0.         0.         0.78891671\n",
      " 0.92819655 0.03788639 0.         0.        ]\n",
      "1 gblinear\n",
      "20022 [1.         0.         0.69520407 0.         0.         1.\n",
      " 0.92237372 0.37826631 0.         0.21481097]\n",
      "1 gblinear\n",
      "20023 [0.28162676 0.         1.         0.30936979 0.65050734 0.69770301\n",
      " 0.64705181 0.58380227 0.         0.        ]\n",
      "0 gbtree\n",
      "20024 [0.6243677  0.         1.         0.53912853 0.         1.\n",
      " 0.87799419 0.61818512 0.         0.0118486 ]\n",
      "1 gblinear\n",
      "20025 [0.74758919 0.20515134 0.90610012 0.19685953 0.14473916 0.82787377\n",
      " 0.82735454 0.48266771 0.08295658 0.18337103]\n",
      "1 gblinear\n",
      "20026 [1.         0.         1.         0.         0.         1.\n",
      " 0.93801522 0.32031601 0.26284555 0.        ]\n",
      "1 gblinear\n",
      "20027 [0.67898486 0.11054782 1.         0.         0.         1.\n",
      " 0.97111493 0.24335959 0.         0.09972314]\n",
      "1 gblinear\n",
      "20028 [0.12403662 0.         1.         0.36475309 0.46632267 1.\n",
      " 0.66650453 0.60271646 0.         0.32753064]\n",
      "0 gbtree\n",
      "20029 [1.         0.00811982 1.         0.25442313 0.         0.93372893\n",
      " 0.8987301  0.32156428 0.         0.22616822]\n",
      "1 gblinear\n",
      "20030 [0.857021   0.         0.77675623 0.32578509 0.5624608  1.\n",
      " 0.7136036  0.56838906 0.         0.        ]\n",
      "1 gblinear\n",
      "20031 [ 1.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -3.46944695e-18  3.97821343e-01  1.00000000e+00  6.36416106e-01\n",
      "  0.00000000e+00  0.00000000e+00]\n",
      "1 gblinear\n",
      "20032 [0.49230604 0.         1.         0.         0.19170665 0.57794894\n",
      " 0.85989853 0.809268   0.         0.        ]\n",
      "0 gbtree\n",
      "20033 [1.         0.22669598 1.         0.07103432 0.         0.50159283\n",
      " 1.         0.36496733 0.         0.02231178]\n",
      "1 gblinear\n",
      "20034 [0.74928786 0.43687479 1.         0.50175981 0.04313601 1.\n",
      " 0.77690851 0.1518313  0.07592315 0.04772338]\n",
      "1 gblinear\n",
      "20035 [0.87423563 0.33083622 0.93001276 0.23471868 0.         1.\n",
      " 0.88012857 0.38553794 0.         0.        ]\n",
      "1 gblinear\n",
      "20036 [0.87262984 0.16684354 0.85862614 0.33856911 0.21299313 1.\n",
      " 0.80452302 0.22506772 0.11140806 0.29616728]\n",
      "1 gblinear\n",
      "20037 [0.33470712 0.158624   1.         0.61928201 0.61339027 1.\n",
      " 0.64733796 0.38269021 0.         0.        ]\n",
      "0 gbtree\n",
      "20038 [1.         0.         1.         0.63419404 0.         1.\n",
      " 1.         0.70155842 0.         0.        ]\n",
      "1 gblinear\n",
      "20039 [1.         0.14416614 1.         0.         0.         1.\n",
      " 1.         0.43673146 0.07410408 0.03520325]\n",
      "1 gblinear\n",
      "20040 [1.         0.23850792 1.         0.47362467 0.         1.\n",
      " 0.79612931 0.5524498  0.         0.19330746]\n",
      "1 gblinear\n",
      "20041 [1.         0.         1.         0.47935862 0.         0.31362831\n",
      " 1.         0.40581917 0.         0.        ]\n",
      "1 gblinear\n",
      "20042 [0.68409071 0.16811108 1.         0.38868396 0.32975048 1.\n",
      " 0.80699541 0.40222735 0.         0.2384402 ]\n",
      "1 gblinear\n",
      "20043 [0.89038645 0.         1.         0.39631125 0.         1.\n",
      " 0.95970465 0.5552192  0.         0.        ]\n",
      "1 gblinear\n",
      "20044 [0.75237404 0.16353733 0.96749493 0.17977343 0.         1.\n",
      " 0.84445531 0.28549088 0.17907688 0.21306606]\n",
      "1 gblinear\n",
      "20045 [0.32807343 0.         0.94182507 0.35595273 0.99710842 0.91344228\n",
      " 0.6195284  0.55185539 0.         0.2453184 ]\n",
      "0 gbtree\n",
      "20046 [0.56225766 0.1835523  0.88180529 0.34110005 0.30854788 0.80691066\n",
      " 0.65383564 0.27898141 0.         0.26000123]\n",
      "1 gblinear\n",
      "20047 [1.         0.         1.         0.15999634 0.         0.72814926\n",
      " 1.         0.54363798 0.         0.        ]\n",
      "1 gblinear\n",
      "20048 [ 1.00000000e+00  0.00000000e+00  1.00000000e+00 -8.67361738e-19\n",
      "  0.00000000e+00  3.43495322e-01  1.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00]\n",
      "1 gblinear\n",
      "20049 [0.76280649 0.         0.94202121 1.         0.         1.\n",
      " 0.86515232 0.4053652  0.         0.03136319]\n",
      "1 gblinear\n",
      "20050 [1.         0.22751642 1.         0.         0.44757553 0.92246362\n",
      " 0.86072535 0.44742944 0.13161589 0.25369526]\n",
      "1 gblinear\n",
      "20051 [1.         0.         1.         0.         0.         0.41485564\n",
      " 1.         0.2513975  0.13945341 0.33333549]\n",
      "1 gblinear\n",
      "20052 [1.         0.         1.         0.         0.88512512 0.\n",
      " 1.         0.14226388 0.         0.        ]\n",
      "1 gblinear\n",
      "20053 [1.         0.         1.         0.         0.51378114 0.43206868\n",
      " 1.         0.14587375 0.         0.10652112]\n",
      "1 gblinear\n",
      "20054 [1.         0.         1.         0.         0.43560533 0.\n",
      " 1.         0.31205763 0.2469448  0.        ]\n",
      "1 gblinear\n",
      "20055 [1.         0.         1.         0.60679182 0.98554667 0.20669306\n",
      " 1.         0.47892594 0.06976594 0.        ]\n",
      "1 gblinear\n",
      "20056 [1.         0.         0.7013321  0.38903721 0.58502634 0.\n",
      " 1.         0.23379705 0.         0.        ]\n",
      "1 gblinear\n",
      "20057 [1.         0.         1.         0.57292771 0.87199494 0.0465369\n",
      " 0.82197436 0.22506983 0.         0.        ]\n",
      "1 gblinear\n",
      "20058 [1.         0.         1.         0.54127671 0.75667079 0.\n",
      " 1.         0.         0.34040942 0.        ]\n",
      "1 gblinear\n",
      "20059 [0.5836808  0.11501731 1.         0.699014   0.         1.\n",
      " 1.         0.         0.         0.        ]\n",
      "1 gblinear\n",
      "20060 [1.         0.         1.         0.47811011 0.59709813 0.\n",
      " 1.         0.15874872 0.         0.        ]\n",
      "1 gblinear\n",
      "20061 [0.06599033 0.         1.         0.7782214  1.         0.51484087\n",
      " 0.68657592 0.57704876 0.         0.        ]\n",
      "0 gbtree\n",
      "20062 [1.         0.         0.92004054 0.26240672 1.         0.35648217\n",
      " 0.94303509 0.29717339 0.39242965 0.        ]\n",
      "1 gblinear\n",
      "20063 [1.         0.         1.         0.7978279  0.         1.\n",
      " 1.         0.0439094  0.42409371 0.        ]\n",
      "1 gblinear\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20064 [0.8379983  0.22128081 1.         0.36716645 0.         1.\n",
      " 1.         0.         0.18214869 0.        ]\n",
      "1 gblinear\n",
      "20065 [1.         0.         1.         0.72296693 0.33195247 0.57870935\n",
      " 0.90724747 0.46389895 0.37408581 0.        ]\n",
      "1 gblinear\n",
      "20066 [0.62528114 0.         1.         0.86202645 0.         1.\n",
      " 0.9733715  0.37179514 0.35185579 0.        ]\n",
      "1 gblinear\n",
      "20067 [1.         0.         1.         1.         0.         0.\n",
      " 1.         0.         0.41626236 0.        ]\n",
      "1 gblinear\n",
      "20068 [1.         0.         1.         0.43205766 0.         0.33155666\n",
      " 1.         0.         0.79041064 0.        ]\n",
      "1 gblinear\n",
      "20069 [1.         0.         1.         1.         0.         0.48218579\n",
      " 1.         0.         0.11718364 0.        ]\n",
      "1 gblinear\n",
      "20070 [1.         0.         1.         1.         0.62398024 0.\n",
      " 0.97185204 0.25259663 0.27475582 0.        ]\n",
      "1 gblinear\n",
      "20071 [1.         0.65960638 1.         1.         0.         0.\n",
      " 1.         0.         0.02256648 0.        ]\n",
      "1 gblinear\n",
      "20072 [1.         0.18357602 0.76179565 1.         0.         0.\n",
      " 1.         0.         0.0122281  0.        ]\n",
      "1 gblinear\n",
      "20073 [1.         0.         1.         1.         0.         1.\n",
      " 0.87275644 1.         0.43371708 0.        ]\n",
      "1 gblinear\n",
      "20074 [1.         0.         1.         0.5235827  0.         1.\n",
      " 0.87125238 0.78598723 0.36509973 0.        ]\n",
      "1 gblinear\n",
      "20075 [1.         0.56276824 1.         1.         0.         1.\n",
      " 0.98155703 0.60728071 0.29092734 0.        ]\n",
      "1 gblinear\n",
      "20076 [1.         0.39064271 1.         1.         0.         0.\n",
      " 0.88375517 0.48259054 0.17035459 0.        ]\n",
      "1 gblinear\n",
      "20077 [0.72600288 0.27470303 1.         0.93130629 0.         1.\n",
      " 0.8332133  0.62874151 0.18965336 0.        ]\n",
      "1 gblinear\n",
      "20078 [ 1.00000000e+00  0.00000000e+00  4.65227924e-01  1.00000000e+00\n",
      " -1.38777878e-17  1.00000000e+00  1.00000000e+00  7.53663324e-01\n",
      "  2.92555737e-01  0.00000000e+00]\n",
      "1 gblinear\n",
      "20079 [1.         0.         0.9242234  1.         0.         1.\n",
      " 1.         0.58352874 0.3354198  0.        ]\n",
      "1 gblinear\n",
      "20080 [1.         0.36772857 0.65374208 0.69448386 0.         1.\n",
      " 1.         0.98171987 0.09525136 0.        ]\n",
      "1 gblinear\n",
      "20081 [1.         0.49457309 1.         0.3652892  0.87011531 0.\n",
      " 0.92148034 0.26871466 0.14807658 0.        ]\n",
      "1 gblinear\n",
      "20082 [1.         0.         0.69768531 0.71112641 1.         1.\n",
      " 0.9597416  0.9327451  0.24186086 0.        ]\n",
      "1 gblinear\n",
      "20083 [1.         0.15421749 0.74165049 0.69861802 0.44379282 1.\n",
      " 0.9202034  0.78848521 0.24920024 0.        ]\n",
      "1 gblinear\n",
      "20084 [1.         0.         1.         0.         1.         1.\n",
      " 1.         0.69029184 0.1605905  0.        ]\n",
      "1 gblinear\n",
      "20085 [0.70884146 0.0861637  1.         0.66427629 0.         1.\n",
      " 0.9257323  0.30118044 0.09593631 0.        ]\n",
      "1 gblinear\n",
      "20086 [1.         0.         1.         1.         1.         1.\n",
      " 1.         0.         0.32892945 0.        ]\n",
      "1 gblinear\n",
      "20087 [0.77989518 0.         1.         0.02877064 1.         1.\n",
      " 1.         0.         0.0905063  0.        ]\n",
      "1 gblinear\n",
      "20088 [1.         0.         0.40096679 0.35217787 1.         1.\n",
      " 1.         0.21818417 0.11068594 0.        ]\n",
      "1 gblinear\n",
      "20089 [1.         0.         1.         0.48030301 1.         1.\n",
      " 0.93843826 0.38341859 0.16087358 0.        ]\n",
      "1 gblinear\n",
      "20090 [1.         0.33961976 1.         1.         0.33342577 0.16447274\n",
      " 0.90146392 0.         0.18730486 0.        ]\n",
      "1 gblinear\n",
      "20091 [0.81142735 0.         1.         0.         0.         1.\n",
      " 1.         0.         0.         0.25152885]\n",
      "1 gblinear\n",
      "20092 [1.         0.41213049 1.         0.07309696 0.         0.\n",
      " 0.88461678 0.         0.         0.27121375]\n",
      "1 gblinear\n",
      "20093 [0.77948761 0.46436781 0.6430842  0.         0.54656089 1.\n",
      " 1.         0.         0.         0.        ]\n",
      "1 gblinear\n",
      "20094 [0.81976407 0.         0.73955229 0.54627536 0.83130784 1.\n",
      " 1.         0.29896025 0.22126002 0.        ]\n",
      "1 gblinear\n",
      "20095 [0.13684535 0.36236576 1.         0.37390215 0.93091611 0.68388376\n",
      " 0.59391389 0.63997379 0.         0.04387803]\n",
      "0 gbtree\n",
      "20096 [1.         0.90359276 0.52059414 1.         0.         1.\n",
      " 1.         0.         0.08921329 0.        ]\n",
      "1 gblinear\n",
      "20097 [1.         0.60784222 1.         1.         0.         1.\n",
      " 1.         0.         0.24153683 0.        ]\n",
      "1 gblinear\n"
     ]
    }
   ],
   "source": [
    "Max_Runs = 100\n",
    "Rand_Seed = 1\n",
    "variables = {}\n",
    "np.random.seed(Rand_Seed)\n",
    "factor_number = len(ParaSpace)\n",
    "for i, label in enumerate(ParaSpace.keys()):\n",
    "    if ParaSpace[label]['Type'] ==\"continuous\":\n",
    "        variables[label] =  collections.OrderedDict({'name': label, \n",
    "                         'type':'float',\n",
    "                         'min': ParaSpace[label]['Range'][0],\n",
    "                         'max': ParaSpace[label]['Range'][1],\n",
    "                         'size': 1})\n",
    "    elif ParaSpace[label]['Type'] ==\"integer\":\n",
    "        variables[label] = collections.OrderedDict({'name': label, \n",
    "                         'type':'int',\n",
    "                         'min': min(ParaSpace[label]['Mapping']),\n",
    "                         'max': max(ParaSpace[label]['Mapping']),\n",
    "                         'size': 1})\n",
    "    elif ParaSpace[label]['Type'] ==\"categorical\":\n",
    "        variables[label] = collections.OrderedDict({'name': label, \n",
    "                         'type':'enum',\n",
    "                         'options': ParaSpace[label]['Mapping'],\n",
    "                         'size': 1})\n",
    "\n",
    "param_unit = []; Params = []; Val = []; Test = []; Time = []\n",
    "file_dir = \"./Benchmark/temp/\" + str(time.time()) + str(np.random.rand(1)[0]) + \"/\"\n",
    "if not os.path.exists(file_dir):\n",
    "    os.makedirs(file_dir)\n",
    "\n",
    "chooser = module.init(file_dir, \"mcmc_iters=10\")\n",
    "vkeys = [k for k in variables]\n",
    "gmap = GridMap([variables[k] for k in vkeys], grid_size)\n",
    "grid = np.asarray(gmap.hypercube_grid(grid_size, 1)) \n",
    "values = np.zeros(grid_size) + np.nan\n",
    "grid_status = np.zeros(grid.shape[0])\n",
    "\n",
    "for i in range(np.int(Max_Runs)):\n",
    "    try:\n",
    "        job_id = spmint_opt(chooser, grid, values, grid_status)\n",
    "    except:\n",
    "        print('Spearmint Early Stop!')\n",
    "        break\n",
    "\n",
    "\n",
    "    if isinstance(job_id, tuple):\n",
    "        (job_id, candidate) = job_id\n",
    "        grid = np.vstack((grid, candidate))\n",
    "        grid_status = np.append(grid_status, 2)\n",
    "        values = np.append(values, np.zeros(1)+np.nan)\n",
    "        job_id = grid.shape[0]-1\n",
    "    else:\n",
    "        candidate = grid[job_id,:]\n",
    "        grid_status[job_id] = 2\n",
    "\n",
    "    print(job_id, candidate)\n",
    "    next_params = gmap.unit_to_list(candidate)\n",
    "    valacc, testacc, tm = obj_func(next_params)\n",
    "    values[job_id] = valacc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmap.unit_to_list(np.array([1.,         0. ,        1.  ,       1. ,        1. ,        0.,\n",
    " 1.    ,     0.     ,    0.13944059 , 0.        ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A contour plot based on a thorough grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid_num = 25\n",
    "xlist = np.linspace(-6, 16, grid_num)\n",
    "ylist = np.linspace(-16, 6, grid_num)\n",
    "X, Y = np.meshgrid(xlist, ylist)\n",
    "Z = np.zeros((grid_num,grid_num))\n",
    "for i, C in enumerate(xlist):\n",
    "    for j, gamma in enumerate(ylist):\n",
    "        estimator = svm.SVC(C=2**C,gamma = 2**gamma)\n",
    "        out = cross_val_score(estimator, x, y, cv = cv, scoring = score_metric)\n",
    "        Z[j,i] = np.mean(out)\n",
    "        \n",
    "levels = [0.2, 0.4, 0.8, 0.9, 0.92, 0.94, 0.96, 0.98, 1.0]\n",
    "cp = plt.contourf(X, Y, Z, levels)\n",
    "plt.colorbar(cp)\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('gamma')\n",
    "plt.scatter(np.log2(clf.logs.loc[:,['C']]), \n",
    "            np.log2(clf.logs.loc[:,['gamma']]), color = \"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2: Xgboost for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import KFold \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "from seqmm.pybayopt import GPEISklearn\n",
    "\n",
    "dt = datasets.load_diabetes()\n",
    "sx = MinMaxScaler()\n",
    "sy = MinMaxScaler()\n",
    "x = sx.fit_transform(dt.data)\n",
    "y = sy.fit_transform(dt.target.reshape([-1,1]))\n",
    "\n",
    "ParaSpace = {'booster':          {'Type': 'categorical', 'Mapping': ['gbtree', 'gblinear']},\n",
    "             'max_depth':        {'Type': 'integer',     'Mapping': np.linspace(2,10,9)}, \n",
    "             'n_estimators':     {'Type': 'integer',     'Mapping': np.linspace(100,500,401)},\n",
    "             'min_child_weight': {'Type': 'integer',     'Mapping': np.linspace(1,100,100)},\n",
    "             'subsample':        {'Type': 'continuous',  'Range': [0, 1],  'Wrapper': lambda x:x},\n",
    "             'colsample_bytree': {'Type': 'continuous',  'Range': [0, 1],  'Wrapper': lambda x:x},\n",
    "             'learning_rate':    {'Type': 'continuous',  'Range': [-5, 0], 'Wrapper': lambda x: 10**x},\n",
    "             'gamma':            {'Type': 'continuous',  'Range': [-5, 0], 'Wrapper': lambda x: 10**x},\n",
    "             'reg_lambda':       {'Type': 'continuous',  'Range': [-5, 0], 'Wrapper': lambda x: 10**x},\n",
    "             'reg_alpha':         {'Type': 'continuous',  'Range': [-5, 0], 'Wrapper': lambda x: 10**x}}\n",
    "\n",
    "estimator = xgb.XGBRegressor()\n",
    "score_metric = make_scorer(mean_squared_error, False)\n",
    "cv = KFold(n_splits=5, random_state=0, shuffle=True)\n",
    "\n",
    "clf = GPEISklearn(estimator, cv, ParaSpace, max_runs = 100, scoring = score_metric, time_out = 30, refit = True, verbose = True)\n",
    "clf.fit(x, y)\n",
    "clf.plot_scores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Example 3: Kmeans for Unsupervised Clustering  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import KFold \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from seqmm.pybayopt import GPEISklearn\n",
    "\n",
    "sx = MinMaxScaler()\n",
    "dt = datasets.load_iris()\n",
    "x = sx.fit_transform(dt.data)\n",
    "y = dt.target.reshape([-1,1])\n",
    "\n",
    "ParaSpace = {'n_clusters':  {'Type': 'integer',    'Mapping': np.linspace(2,9,8)}, \n",
    "             'tol':         {'Type': 'continuous', 'Range': [-6, -3], 'Wrapper': lambda x: 10**x}}\n",
    "\n",
    "estimator = KMeans()\n",
    "cv = KFold(n_splits=5, random_state=0, shuffle=True)\n",
    "\n",
    "clf = GPEISklearn(estimator, cv, ParaSpace, max_runs = 100, refit = True, verbose = True)\n",
    "clf.fit(x, y)\n",
    "clf.plot_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "r-tensorflow",
   "language": "python",
   "name": "r-tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
